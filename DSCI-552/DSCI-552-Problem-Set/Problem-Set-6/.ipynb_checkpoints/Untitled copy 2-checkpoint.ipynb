{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"datasets/ps6_trainvalid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>pressure</th>\n",
       "      <th>weather</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>wind_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-10-01 12:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-10-01 13:00:00</td>\n",
       "      <td>291.870000</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>mist</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-10-01 14:00:00</td>\n",
       "      <td>291.868186</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>sky is clear</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-10-01 15:00:00</td>\n",
       "      <td>291.862844</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>sky is clear</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-10-01 16:00:00</td>\n",
       "      <td>291.857503</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>sky is clear</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime  temperature  humidity  pressure       weather  \\\n",
       "0  2012-10-01 12:00:00          NaN       NaN       NaN           NaN   \n",
       "1  2012-10-01 13:00:00   291.870000      88.0    1013.0          mist   \n",
       "2  2012-10-01 14:00:00   291.868186      88.0    1013.0  sky is clear   \n",
       "3  2012-10-01 15:00:00   291.862844      88.0    1013.0  sky is clear   \n",
       "4  2012-10-01 16:00:00   291.857503      88.0    1013.0  sky is clear   \n",
       "\n",
       "   wind_direction  wind_speed  \n",
       "0             NaN         NaN  \n",
       "1             0.0         0.0  \n",
       "2             0.0         0.0  \n",
       "3             0.0         0.0  \n",
       "4             0.0         0.0  "
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime          datetime64[ns]\n",
      "temperature              float64\n",
      "humidity                 float64\n",
      "pressure                 float64\n",
      "weather                   object\n",
      "wind_direction           float64\n",
      "wind_speed               float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First using only temperature to predict 'future'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model which can only forecast the 'next' hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 7)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dealing with missing data\n",
    "missing_data = df[pd.isnull(df[\"temperature\"])]\n",
    "missing_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop those data\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the evolution of temperature over time\n",
    "def plot_series(time, series, format=\"-\", start=0, end=None, label=None):\n",
    "    plt.plot(time[start:end], series[start:end], format, label=label)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    if label:\n",
    "        plt.legend(fontsize=14)\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "time = df['datetime']\n",
    "temp_values = df['temperature'].values \n",
    "plot_series(time, temp_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44671,)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44671,)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([291.87      , 291.86818552, 291.86284446, ..., 296.51      ,\n",
       "       297.09      , 296.69      ])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate data preparation\n",
    "from numpy import array\n",
    "\n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 24 * 5\n",
    "\n",
    "X, y = split_sequence(temp_values, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44551, 120)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44551,)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training X: (31185, 120)\n",
      "Shape of validation X: (13366, 120)\n",
      "Shape of training y: (31185,)\n",
      "Shape of validation y: (13366,)\n"
     ]
    }
   ],
   "source": [
    "# Split training and validation set\n",
    "n = len(X)\n",
    "train_X = X[0:int(n*0.7),:]\n",
    "val_X = X[int(n*0.7):,:]\n",
    "\n",
    "train_y = y[0:int(n*0.7)]\n",
    "val_y = y[int(n*0.7):]\n",
    "\n",
    "print(\"Shape of training X: {}\".format(train_X.shape))\n",
    "print(\"Shape of validation X: {}\".format(val_X.shape))\n",
    "\n",
    "print(\"Shape of training y: {}\".format(train_y.shape))\n",
    "print(\"Shape of validation y: {}\".format(val_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 8)                 968       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 977\n",
      "Trainable params: 977\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "Linear_model = Sequential()\n",
    "Linear_model.add(Dense(8, input_shape =[n_steps], activation='relu'))\n",
    "Linear_model.add(Dense(1))\n",
    "Linear_model.compile(loss='mse', optimizer='adam')\n",
    "print(Linear_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "780/780 - 1s - loss: 75.7868 - val_loss: 31.2741\n",
      "Epoch 2/100\n",
      "780/780 - 1s - loss: 27.8272 - val_loss: 17.9935\n",
      "Epoch 3/100\n",
      "780/780 - 1s - loss: 18.7470 - val_loss: 13.7391\n",
      "Epoch 4/100\n",
      "780/780 - 1s - loss: 15.9092 - val_loss: 10.0327\n",
      "Epoch 5/100\n",
      "780/780 - 1s - loss: 15.0621 - val_loss: 9.2044\n",
      "Epoch 6/100\n",
      "780/780 - 1s - loss: 14.0138 - val_loss: 19.8028\n",
      "Epoch 7/100\n",
      "780/780 - 1s - loss: 14.2355 - val_loss: 9.9072\n",
      "Epoch 8/100\n",
      "780/780 - 1s - loss: 12.0056 - val_loss: 9.6805\n",
      "Epoch 9/100\n",
      "780/780 - 1s - loss: 12.6859 - val_loss: 11.3424\n",
      "Epoch 10/100\n",
      "780/780 - 1s - loss: 10.7037 - val_loss: 6.7480\n",
      "Epoch 11/100\n",
      "780/780 - 1s - loss: 10.3755 - val_loss: 7.6054\n",
      "Epoch 12/100\n",
      "780/780 - 1s - loss: 10.3465 - val_loss: 6.2010\n",
      "Epoch 13/100\n",
      "780/780 - 1s - loss: 9.4626 - val_loss: 7.3828\n",
      "Epoch 14/100\n",
      "780/780 - 2s - loss: 8.6787 - val_loss: 13.1639\n",
      "Epoch 15/100\n",
      "780/780 - 1s - loss: 8.6375 - val_loss: 7.3465\n",
      "Epoch 16/100\n",
      "780/780 - 1s - loss: 9.5316 - val_loss: 5.6560\n",
      "Epoch 17/100\n",
      "780/780 - 1s - loss: 8.4118 - val_loss: 10.4066\n",
      "Epoch 18/100\n",
      "780/780 - 1s - loss: 8.5914 - val_loss: 6.0759\n",
      "Epoch 19/100\n",
      "780/780 - 1s - loss: 7.7873 - val_loss: 9.2633\n",
      "Epoch 20/100\n",
      "780/780 - 1s - loss: 7.3060 - val_loss: 6.8154\n",
      "Epoch 21/100\n",
      "780/780 - 1s - loss: 7.7458 - val_loss: 13.4685\n",
      "Epoch 22/100\n",
      "780/780 - 1s - loss: 7.3608 - val_loss: 6.5037\n",
      "Epoch 23/100\n",
      "780/780 - 1s - loss: 7.1172 - val_loss: 10.5357\n",
      "Epoch 24/100\n",
      "780/780 - 1s - loss: 6.9197 - val_loss: 4.0394\n",
      "Epoch 25/100\n",
      "780/780 - 1s - loss: 6.7584 - val_loss: 4.2090\n",
      "Epoch 26/100\n",
      "780/780 - 1s - loss: 6.6152 - val_loss: 4.6444\n",
      "Epoch 27/100\n",
      "780/780 - 1s - loss: 6.2848 - val_loss: 4.1816\n",
      "Epoch 28/100\n",
      "780/780 - 1s - loss: 6.3093 - val_loss: 7.1784\n",
      "Epoch 29/100\n",
      "780/780 - 1s - loss: 6.4743 - val_loss: 3.9759\n",
      "Epoch 30/100\n",
      "780/780 - 1s - loss: 5.7625 - val_loss: 3.5742\n",
      "Epoch 31/100\n",
      "780/780 - 1s - loss: 5.7729 - val_loss: 9.4817\n",
      "Epoch 32/100\n",
      "780/780 - 1s - loss: 5.6100 - val_loss: 3.5398\n",
      "Epoch 33/100\n",
      "780/780 - 1s - loss: 6.0276 - val_loss: 3.7131\n",
      "Epoch 34/100\n",
      "780/780 - 1s - loss: 5.5756 - val_loss: 3.7585\n",
      "Epoch 35/100\n",
      "780/780 - 1s - loss: 5.4142 - val_loss: 4.1217\n",
      "Epoch 36/100\n",
      "780/780 - 1s - loss: 5.7404 - val_loss: 3.3362\n",
      "Epoch 37/100\n",
      "780/780 - 1s - loss: 5.3348 - val_loss: 5.4207\n",
      "Epoch 38/100\n",
      "780/780 - 1s - loss: 5.5422 - val_loss: 7.8309\n",
      "Epoch 39/100\n",
      "780/780 - 1s - loss: 5.1933 - val_loss: 6.6988\n",
      "Epoch 40/100\n",
      "780/780 - 1s - loss: 4.7691 - val_loss: 3.1321\n",
      "Epoch 41/100\n",
      "780/780 - 1s - loss: 5.1973 - val_loss: 4.1851\n",
      "Epoch 42/100\n",
      "780/780 - 1s - loss: 4.9684 - val_loss: 3.1038\n",
      "Epoch 43/100\n",
      "780/780 - 1s - loss: 4.7353 - val_loss: 3.0724\n",
      "Epoch 44/100\n",
      "780/780 - 1s - loss: 5.3081 - val_loss: 7.0282\n",
      "Epoch 45/100\n",
      "780/780 - 1s - loss: 4.4646 - val_loss: 3.2156\n",
      "Epoch 46/100\n",
      "780/780 - 1s - loss: 5.1322 - val_loss: 6.2602\n",
      "Epoch 47/100\n",
      "780/780 - 1s - loss: 4.9446 - val_loss: 7.0368\n",
      "Epoch 48/100\n",
      "780/780 - 1s - loss: 4.6504 - val_loss: 3.7457\n",
      "Epoch 49/100\n",
      "780/780 - 1s - loss: 4.7483 - val_loss: 4.5307\n",
      "Epoch 50/100\n",
      "780/780 - 1s - loss: 4.6263 - val_loss: 3.0150\n",
      "Epoch 51/100\n",
      "780/780 - 1s - loss: 4.6042 - val_loss: 3.9033\n",
      "Epoch 52/100\n",
      "780/780 - 1s - loss: 4.3930 - val_loss: 4.4041\n",
      "Epoch 53/100\n",
      "780/780 - 1s - loss: 4.5686 - val_loss: 2.8026\n",
      "Epoch 54/100\n",
      "780/780 - 1s - loss: 4.1395 - val_loss: 4.6771\n",
      "Epoch 55/100\n",
      "780/780 - 1s - loss: 4.1876 - val_loss: 2.9690\n",
      "Epoch 56/100\n",
      "780/780 - 1s - loss: 4.5425 - val_loss: 3.0785\n",
      "Epoch 57/100\n",
      "780/780 - 1s - loss: 3.8837 - val_loss: 2.9883\n",
      "Epoch 58/100\n",
      "780/780 - 1s - loss: 4.4016 - val_loss: 2.9746\n",
      "Epoch 59/100\n",
      "780/780 - 1s - loss: 4.3657 - val_loss: 3.1453\n",
      "Epoch 60/100\n",
      "780/780 - 1s - loss: 4.2391 - val_loss: 3.6221\n",
      "Epoch 61/100\n",
      "780/780 - 1s - loss: 4.0039 - val_loss: 3.2626\n",
      "Epoch 62/100\n",
      "780/780 - 1s - loss: 4.0526 - val_loss: 3.1505\n",
      "Epoch 63/100\n",
      "780/780 - 1s - loss: 4.2568 - val_loss: 4.6243\n",
      "Epoch 64/100\n",
      "780/780 - 1s - loss: 4.4216 - val_loss: 2.7338\n",
      "Epoch 65/100\n",
      "780/780 - 1s - loss: 3.9162 - val_loss: 4.9836\n",
      "Epoch 66/100\n",
      "780/780 - 1s - loss: 3.9341 - val_loss: 4.0377\n",
      "Epoch 67/100\n",
      "780/780 - 1s - loss: 3.7168 - val_loss: 5.1868\n",
      "Epoch 68/100\n",
      "780/780 - 1s - loss: 3.9435 - val_loss: 2.7472\n",
      "Epoch 69/100\n",
      "780/780 - 1s - loss: 3.8995 - val_loss: 2.6367\n",
      "Epoch 70/100\n",
      "780/780 - 1s - loss: 3.8149 - val_loss: 3.4503\n",
      "Epoch 71/100\n",
      "780/780 - 1s - loss: 4.0244 - val_loss: 3.2033\n",
      "Epoch 72/100\n",
      "780/780 - 1s - loss: 4.1883 - val_loss: 3.6727\n",
      "Epoch 73/100\n",
      "780/780 - 1s - loss: 3.8990 - val_loss: 2.9952\n",
      "Epoch 74/100\n",
      "780/780 - 1s - loss: 4.0497 - val_loss: 4.3074\n",
      "Epoch 75/100\n",
      "780/780 - 1s - loss: 3.7910 - val_loss: 3.3090\n",
      "Epoch 76/100\n",
      "780/780 - 1s - loss: 3.7527 - val_loss: 3.4714\n",
      "Epoch 77/100\n",
      "780/780 - 1s - loss: 3.6221 - val_loss: 2.4839\n",
      "Epoch 78/100\n",
      "780/780 - 1s - loss: 3.6847 - val_loss: 2.4991\n",
      "Epoch 79/100\n",
      "780/780 - 1s - loss: 3.7119 - val_loss: 9.2021\n",
      "Epoch 80/100\n",
      "780/780 - 1s - loss: 3.7915 - val_loss: 7.3422\n",
      "Epoch 81/100\n",
      "780/780 - 1s - loss: 3.7400 - val_loss: 3.4621\n",
      "Epoch 82/100\n",
      "780/780 - 1s - loss: 3.3139 - val_loss: 3.0398\n",
      "Epoch 83/100\n",
      "780/780 - 1s - loss: 3.6562 - val_loss: 6.8356\n",
      "Epoch 84/100\n",
      "780/780 - 1s - loss: 3.7359 - val_loss: 3.8608\n",
      "Epoch 85/100\n",
      "780/780 - 1s - loss: 3.3759 - val_loss: 3.8685\n",
      "Epoch 86/100\n",
      "780/780 - 1s - loss: 3.4463 - val_loss: 10.2656\n",
      "Epoch 87/100\n",
      "780/780 - 1s - loss: 3.9555 - val_loss: 3.0060\n",
      "Epoch 88/100\n",
      "780/780 - 1s - loss: 3.3265 - val_loss: 2.5245\n",
      "Epoch 89/100\n",
      "780/780 - 1s - loss: 3.4394 - val_loss: 3.9332\n",
      "Epoch 90/100\n",
      "780/780 - 1s - loss: 3.4717 - val_loss: 8.3585\n",
      "Epoch 91/100\n",
      "780/780 - 1s - loss: 3.8508 - val_loss: 7.0061\n",
      "Epoch 92/100\n",
      "780/780 - 1s - loss: 3.2065 - val_loss: 2.3490\n",
      "Epoch 93/100\n",
      "780/780 - 1s - loss: 3.5046 - val_loss: 2.3289\n",
      "Epoch 94/100\n",
      "780/780 - 1s - loss: 3.2699 - val_loss: 2.5873\n",
      "Epoch 95/100\n",
      "780/780 - 1s - loss: 3.3176 - val_loss: 2.7543\n",
      "Epoch 96/100\n",
      "780/780 - 1s - loss: 3.4954 - val_loss: 7.4778\n",
      "Epoch 97/100\n",
      "780/780 - 1s - loss: 3.7178 - val_loss: 2.4120\n",
      "Epoch 98/100\n",
      "780/780 - 1s - loss: 3.3843 - val_loss: 2.4234\n",
      "Epoch 99/100\n",
      "780/780 - 1s - loss: 3.2134 - val_loss: 2.2752\n",
      "Epoch 100/100\n",
      "780/780 - 1s - loss: 3.3957 - val_loss: 2.3699\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb71eb796d0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "# Linear_model.fit(train_X, train_y, epochs=100, verbose=0)\n",
    "Linear_model.fit(train_X, train_y, epochs=100, validation_split=0.2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13366,)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = val_y\n",
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13366, 1)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = Linear_model.predict(val_X)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error is: 1.73\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "valScore = mean_squared_error(y_true, y_pred)\n",
    "print('Mean Squared Error is: %.2f' % (valScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN(LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 1\n",
    "\n",
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44551, 120, 1)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44551,)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training X: (31185, 120, 1)\n",
      "Shape of validation X: (13366, 120, 1)\n",
      "Shape of training y: (31185,)\n",
      "Shape of validation y: (13366,)\n"
     ]
    }
   ],
   "source": [
    "# Split training and validation set\n",
    "n = len(X)\n",
    "train_X = X[0:int(n*0.7),:]\n",
    "val_X = X[int(n*0.7):,:]\n",
    "\n",
    "train_y = y[0:int(n*0.7)]\n",
    "val_y = y[int(n*0.7):]\n",
    "\n",
    "print(\"Shape of training X: {}\".format(train_X.shape))\n",
    "print(\"Shape of validation X: {}\".format(val_X.shape))\n",
    "\n",
    "print(\"Shape of training y: {}\".format(train_y.shape))\n",
    "print(\"Shape of validation y: {}\".format(val_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "train_X_mean = train_X.mean()\n",
    "train_X_std = train_X.std()\n",
    "\n",
    "train_y_mean = train_y.mean()\n",
    "train_y_std = train_y.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_train_X = (train_X - train_X_mean)/train_X_std\n",
    "normalized_val_X = (val_X - train_X_mean)/train_X_std\n",
    "\n",
    "normalized_train_y = (train_y - train_y_mean)/train_y_std\n",
    "normalized_val_y = (val_y - train_y_mean)/train_y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 10)                480       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 491\n",
      "Trainable params: 491\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "RNN_model = Sequential()\n",
    "RNN_model.add(LSTM(10, activation='relu', input_shape=(n_steps, n_features)))\n",
    "RNN_model.add(Dense(1))\n",
    "RNN_model.compile(optimizer='adam', loss='mse')\n",
    "print(RNN_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "780/780 - 34s - loss: 1206.0881 - val_loss: 175.5948\n",
      "Epoch 2/100\n",
      "780/780 - 32s - loss: 169.0961 - val_loss: 166.3976\n",
      "Epoch 3/100\n",
      "780/780 - 32s - loss: 159.7646 - val_loss: 157.4647\n",
      "Epoch 4/100\n",
      "780/780 - 35s - loss: 150.5395 - val_loss: 148.1268\n",
      "Epoch 5/100\n",
      "780/780 - 37s - loss: 140.9213 - val_loss: 138.0311\n",
      "Epoch 6/100\n",
      "780/780 - 36s - loss: 130.6278 - val_loss: 126.9684\n",
      "Epoch 7/100\n",
      "780/780 - 33s - loss: 119.5028 - val_loss: 114.8538\n",
      "Epoch 8/100\n",
      "780/780 - 35s - loss: 107.6095 - val_loss: 101.9117\n",
      "Epoch 9/100\n",
      "780/780 - 32s - loss: 95.2959 - val_loss: 88.6389\n",
      "Epoch 10/100\n",
      "780/780 - 36s - loss: 83.1238 - val_loss: 75.7276\n",
      "Epoch 11/100\n",
      "780/780 - 35s - loss: 71.6401 - val_loss: 63.7342\n",
      "Epoch 12/100\n",
      "780/780 - 36s - loss: 61.2115 - val_loss: 53.0259\n",
      "Epoch 13/100\n",
      "780/780 - 41s - loss: 51.9618 - val_loss: 43.6928\n",
      "Epoch 14/100\n",
      "780/780 - 37s - loss: 43.8313 - val_loss: 35.6734\n",
      "Epoch 15/100\n",
      "780/780 - 39s - loss: 36.7138 - val_loss: 28.8466\n",
      "Epoch 16/100\n",
      "780/780 - 36s - loss: 30.5216 - val_loss: 23.1285\n",
      "Epoch 17/100\n",
      "780/780 - 32s - loss: 25.1627 - val_loss: 18.3750\n",
      "Epoch 18/100\n",
      "780/780 - 35s - loss: 20.5572 - val_loss: 14.5036\n",
      "Epoch 19/100\n",
      "780/780 - 38s - loss: 16.6516 - val_loss: 11.4328\n",
      "Epoch 20/100\n",
      "780/780 - 36s - loss: 13.3815 - val_loss: 9.0523\n",
      "Epoch 21/100\n",
      "780/780 - 33s - loss: 10.6923 - val_loss: 7.2820\n",
      "Epoch 22/100\n",
      "780/780 - 32s - loss: 8.5344 - val_loss: 5.9837\n",
      "Epoch 23/100\n",
      "780/780 - 33s - loss: 6.8330 - val_loss: 5.0206\n",
      "Epoch 24/100\n",
      "780/780 - 32s - loss: 5.4929 - val_loss: 4.2801\n",
      "Epoch 25/100\n",
      "780/780 - 32s - loss: 4.4254 - val_loss: 3.6574\n",
      "Epoch 26/100\n",
      "780/780 - 33s - loss: 3.5710 - val_loss: 3.0959\n",
      "Epoch 27/100\n",
      "780/780 - 33s - loss: 2.7025 - val_loss: 1.4151\n",
      "Epoch 28/100\n",
      "780/780 - 35s - loss: 0.4691 - val_loss: 0.1007\n",
      "Epoch 29/100\n",
      "780/780 - 32s - loss: 0.1379 - val_loss: 0.0848\n",
      "Epoch 30/100\n",
      "780/780 - 32s - loss: 0.1087 - val_loss: 0.0776\n",
      "Epoch 31/100\n",
      "780/780 - 36s - loss: 0.0945 - val_loss: 0.0716\n",
      "Epoch 32/100\n",
      "780/780 - 32s - loss: 0.0850 - val_loss: 0.0694\n",
      "Epoch 33/100\n",
      "780/780 - 31s - loss: 0.0773 - val_loss: 0.0657\n",
      "Epoch 34/100\n",
      "780/780 - 31s - loss: 0.0712 - val_loss: 0.0637\n",
      "Epoch 35/100\n",
      "780/780 - 32s - loss: 0.0662 - val_loss: 0.0609\n",
      "Epoch 36/100\n",
      "780/780 - 32s - loss: 0.0626 - val_loss: 0.0593\n",
      "Epoch 37/100\n",
      "780/780 - 31s - loss: 0.0596 - val_loss: 0.0594\n",
      "Epoch 38/100\n",
      "780/780 - 34s - loss: 0.0571 - val_loss: 0.0571\n",
      "Epoch 39/100\n",
      "780/780 - 31s - loss: 0.0552 - val_loss: 0.0576\n",
      "Epoch 40/100\n",
      "780/780 - 31s - loss: 0.0533 - val_loss: 0.0563\n",
      "Epoch 41/100\n",
      "780/780 - 31s - loss: 0.0520 - val_loss: 0.0555\n",
      "Epoch 42/100\n",
      "780/780 - 31s - loss: 0.0507 - val_loss: 0.0546\n",
      "Epoch 43/100\n",
      "780/780 - 31s - loss: 0.0495 - val_loss: 0.0551\n",
      "Epoch 44/100\n",
      "780/780 - 31s - loss: 0.0483 - val_loss: 0.0552\n",
      "Epoch 45/100\n",
      "780/780 - 33s - loss: 0.0477 - val_loss: 0.0548\n",
      "Epoch 46/100\n",
      "780/780 - 34s - loss: 0.0469 - val_loss: 0.0528\n",
      "Epoch 47/100\n",
      "780/780 - 32s - loss: 0.0462 - val_loss: 0.0548\n",
      "Epoch 48/100\n",
      "780/780 - 31s - loss: 0.0454 - val_loss: 0.0561\n",
      "Epoch 49/100\n",
      "780/780 - 34s - loss: 0.0454 - val_loss: 0.0530\n",
      "Epoch 50/100\n",
      "780/780 - 34s - loss: 0.0448 - val_loss: 0.0533\n",
      "Epoch 51/100\n",
      "780/780 - 31s - loss: 0.0441 - val_loss: 0.0510\n",
      "Epoch 52/100\n",
      "780/780 - 31s - loss: 0.0436 - val_loss: 0.0505\n",
      "Epoch 53/100\n",
      "780/780 - 32s - loss: 0.0436 - val_loss: 0.0511\n",
      "Epoch 54/100\n",
      "780/780 - 33s - loss: 0.0427 - val_loss: 0.0488\n",
      "Epoch 55/100\n",
      "780/780 - 32s - loss: 0.0424 - val_loss: 0.0482\n",
      "Epoch 56/100\n",
      "780/780 - 32s - loss: 0.0421 - val_loss: 0.0513\n",
      "Epoch 57/100\n",
      "780/780 - 32s - loss: 0.0416 - val_loss: 0.0479\n",
      "Epoch 58/100\n",
      "780/780 - 32s - loss: 0.0417 - val_loss: 0.0499\n",
      "Epoch 59/100\n",
      "780/780 - 32s - loss: 0.0407 - val_loss: 0.0474\n",
      "Epoch 60/100\n",
      "780/780 - 31s - loss: 0.0405 - val_loss: 0.0469\n",
      "Epoch 61/100\n",
      "780/780 - 31s - loss: 0.0403 - val_loss: 0.0465\n",
      "Epoch 62/100\n",
      "780/780 - 32s - loss: 0.0400 - val_loss: 0.0451\n",
      "Epoch 63/100\n",
      "780/780 - 32s - loss: 0.0399 - val_loss: 0.0449\n",
      "Epoch 64/100\n",
      "780/780 - 31s - loss: 0.0399 - val_loss: 0.0455\n",
      "Epoch 65/100\n",
      "780/780 - 32s - loss: 0.0393 - val_loss: 0.0456\n",
      "Epoch 66/100\n",
      "780/780 - 32s - loss: 0.0394 - val_loss: 0.0455\n",
      "Epoch 67/100\n",
      "780/780 - 31s - loss: 0.0390 - val_loss: 0.0478\n",
      "Epoch 68/100\n",
      "780/780 - 31s - loss: 0.0391 - val_loss: 0.0452\n",
      "Epoch 69/100\n",
      "780/780 - 31s - loss: 0.0387 - val_loss: 0.0470\n",
      "Epoch 70/100\n",
      "780/780 - 31s - loss: 0.0386 - val_loss: 0.0462\n",
      "Epoch 71/100\n",
      "780/780 - 32s - loss: 0.0381 - val_loss: 0.0459\n",
      "Epoch 72/100\n",
      "780/780 - 32s - loss: 0.0381 - val_loss: 0.0457\n",
      "Epoch 73/100\n",
      "780/780 - 31s - loss: 0.0381 - val_loss: 0.0463\n",
      "Epoch 74/100\n",
      "780/780 - 32s - loss: 0.0380 - val_loss: 0.0462\n",
      "Epoch 75/100\n",
      "780/780 - 31s - loss: 0.0378 - val_loss: 0.0455\n",
      "Epoch 76/100\n",
      "780/780 - 32s - loss: 0.0378 - val_loss: 0.0444\n",
      "Epoch 77/100\n",
      "780/780 - 31s - loss: 0.0376 - val_loss: 0.0441\n",
      "Epoch 78/100\n",
      "780/780 - 31s - loss: 0.0375 - val_loss: 0.0451\n",
      "Epoch 79/100\n",
      "780/780 - 31s - loss: 0.0374 - val_loss: 0.0441\n",
      "Epoch 80/100\n",
      "780/780 - 32s - loss: 0.0374 - val_loss: 0.0441\n",
      "Epoch 81/100\n",
      "780/780 - 31s - loss: 0.0373 - val_loss: 0.0459\n",
      "Epoch 82/100\n",
      "780/780 - 31s - loss: 0.0372 - val_loss: 0.0441\n",
      "Epoch 83/100\n",
      "780/780 - 31s - loss: 0.0371 - val_loss: 0.0433\n",
      "Epoch 84/100\n",
      "780/780 - 32s - loss: 0.0365 - val_loss: 0.0439\n",
      "Epoch 85/100\n",
      "780/780 - 31s - loss: 0.0366 - val_loss: 0.0431\n",
      "Epoch 86/100\n",
      "780/780 - 32s - loss: 0.0369 - val_loss: 0.0431\n",
      "Epoch 87/100\n",
      "780/780 - 32s - loss: 0.0367 - val_loss: 0.0434\n",
      "Epoch 88/100\n",
      "780/780 - 31s - loss: 0.0363 - val_loss: 0.0437\n",
      "Epoch 89/100\n",
      "780/780 - 31s - loss: 0.0365 - val_loss: 0.0434\n",
      "Epoch 90/100\n",
      "780/780 - 33s - loss: 0.0364 - val_loss: 0.0439\n",
      "Epoch 91/100\n",
      "780/780 - 32s - loss: 0.0363 - val_loss: 0.0453\n",
      "Epoch 92/100\n",
      "780/780 - 32s - loss: 0.0362 - val_loss: 0.0450\n",
      "Epoch 93/100\n",
      "780/780 - 32s - loss: 0.0360 - val_loss: 0.0427\n",
      "Epoch 94/100\n",
      "780/780 - 32s - loss: 0.0359 - val_loss: 0.0435\n",
      "Epoch 95/100\n",
      "780/780 - 32s - loss: 0.0356 - val_loss: 0.0456\n",
      "Epoch 96/100\n",
      "780/780 - 32s - loss: 0.0356 - val_loss: 0.0455\n",
      "Epoch 97/100\n",
      "780/780 - 32s - loss: 0.0354 - val_loss: 0.0459\n",
      "Epoch 98/100\n",
      "780/780 - 32s - loss: 0.0353 - val_loss: 0.0489\n",
      "Epoch 99/100\n",
      "780/780 - 32s - loss: 0.0350 - val_loss: 0.0427\n",
      "Epoch 100/100\n",
      "780/780 - 32s - loss: 0.0350 - val_loss: 0.0432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb718c615e0>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "RNN_model.fit(normalized_train_X, normalized_train_y, epochs=100, validation_split=0.2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13366,)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = val_y\n",
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13366, 1)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_y_pred = RNN_model.predict(normalized_val_X)\n",
    "y_pred = normalized_y_pred * train_y.std() + train_y.mean()\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error is: 1.03\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "valScore = mean_squared_error(y_true, y_pred)\n",
    "print('Mean Squared Error is: %.2f' % (valScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model which can forecast the 'future' 24 hours, 72 hours, ......."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting future 24 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 20 30] [40 50]\n",
      "[30 40 50] [60 70]\n",
      "[50 60 70] [80 90]\n"
     ]
    }
   ],
   "source": [
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    # for i in range(len(sequence)):\n",
    "    #     # find the end of this pattern\n",
    "    #     end_ix = i + n_steps_in\n",
    "    #     out_end_ix = end_ix + n_steps_out\n",
    "    #     # check if we are beyond the sequence\n",
    "    #     if out_end_ix > len(sequence):\n",
    "    #         break\n",
    "    #     # gather input and output parts of the pattern\n",
    "    #     seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n",
    "    #     X.append(seq_x)\n",
    "    #     y.append(seq_y)\n",
    "    i = 0\n",
    "    while i + n_steps_in + n_steps_out <= len(sequence):\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = i + n_steps_in + n_steps_out\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "        i += n_steps_out\n",
    "    return np.array(X), np.array(y)\n",
    " \n",
    "# define input sequence\n",
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "# choose a number of time steps\n",
    "n_steps_in, n_steps_out = 3, 2\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps_in, n_steps_out)\n",
    "# summarize the data\n",
    "for i in range(len(X)):\n",
    "\tprint(X[i], y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a number of time steps\n",
    "n_steps_in, n_steps_out = 24*5, 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_sequence(temp_values, n_steps_in, n_steps_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1856, 120)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using humidity and pressure to predict temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequences)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the dataset\n",
    "\t\tif end_ix > len(sequences)-1:\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix, :]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y[0])\n",
    "\treturn array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['temperature', 'humidity', 'pressure']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = list(df)[1:4]\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45013, 7)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dealing with missing data\n",
    "missing_data = df[pd.isnull(df[cols])]\n",
    "missing_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop those data\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 24 * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_1 = ['temperature', 'humidity']\n",
    "cols_2 = ['temperature', 'pressure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df[cols_1].values\n",
    "df_2 = df[cols_2].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using temperature and humidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44551, 120, 2) (44551,)\n"
     ]
    }
   ],
   "source": [
    "X, y = split_sequences(df_1, n_steps)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = X.shape[2]\n",
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training X: (31185, 120, 2)\n",
      "Shape of validation X: (13366, 120, 2)\n",
      "Shape of training y: (31185,)\n",
      "Shape of validation y: (13366,)\n"
     ]
    }
   ],
   "source": [
    "# Split training and validation set\n",
    "n = len(X)\n",
    "train_X = X[0:int(n*0.7),:]\n",
    "val_X = X[int(n*0.7):,:]\n",
    "\n",
    "train_y = y[0:int(n*0.7)]\n",
    "val_y = y[int(n*0.7):]\n",
    "\n",
    "print(\"Shape of training X: {}\".format(train_X.shape))\n",
    "print(\"Shape of validation X: {}\".format(val_X.shape))\n",
    "\n",
    "print(\"Shape of training y: {}\".format(train_y.shape))\n",
    "print(\"Shape of validation y: {}\".format(val_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "train_X_mean = train_X.mean()\n",
    "train_X_std = train_X.std()\n",
    "\n",
    "train_y_mean = train_y.mean()\n",
    "train_y_std = train_y.std()\n",
    "\n",
    "normalized_train_X = (train_X - train_X_mean)/train_X_std\n",
    "normalized_val_X = (val_X - train_X_mean)/train_X_std\n",
    "\n",
    "normalized_train_y = (train_y - train_y_mean)/train_y_std\n",
    "normalized_val_y = (val_y - train_y_mean)/train_y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([290.13227985,  61.47520416])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = np.mean(train_X, axis=0)\n",
    "train_X_mean = np.mean(new_df, axis=0)\n",
    "train_X_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = np.std(train_X, axis=0)\n",
    "train_X_std = np.std(new_df, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290.1231005220199"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 10)                520       \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 531\n",
      "Trainable params: 531\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "RNN_model_2 = Sequential()\n",
    "RNN_model_2.add(LSTM(10, activation='relu', input_shape=(n_steps, n_features)))\n",
    "RNN_model_2.add(Dense(1))\n",
    "RNN_model_2.compile(optimizer='adam', loss='mse')\n",
    "print(RNN_model_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "780/780 - 40s - loss: 1.0628 - val_loss: 0.8864\n",
      "Epoch 2/100\n",
      "780/780 - 34s - loss: 0.8661 - val_loss: 0.7579\n",
      "Epoch 3/100\n",
      "780/780 - 33s - loss: 0.8196 - val_loss: 0.7651\n",
      "Epoch 4/100\n",
      "780/780 - 33s - loss: 0.7566 - val_loss: 0.6854\n",
      "Epoch 5/100\n",
      "780/780 - 33s - loss: 0.6630 - val_loss: 0.5854\n",
      "Epoch 6/100\n",
      "780/780 - 33s - loss: 0.6105 - val_loss: 0.6007\n",
      "Epoch 7/100\n",
      "780/780 - 33s - loss: 0.5689 - val_loss: 0.5213\n",
      "Epoch 8/100\n",
      "780/780 - 33s - loss: 0.4805 - val_loss: 0.4093\n",
      "Epoch 9/100\n",
      "780/780 - 33s - loss: 0.6237 - val_loss: 0.5021\n",
      "Epoch 10/100\n",
      "780/780 - 33s - loss: 3.5239 - val_loss: 0.8978\n",
      "Epoch 11/100\n",
      "780/780 - 33s - loss: 0.8208 - val_loss: 0.7928\n",
      "Epoch 12/100\n",
      "780/780 - 33s - loss: 0.7976 - val_loss: 0.7765\n",
      "Epoch 13/100\n",
      "780/780 - 33s - loss: 0.7833 - val_loss: 0.7589\n",
      "Epoch 14/100\n",
      "780/780 - 33s - loss: 0.7682 - val_loss: 0.7405\n",
      "Epoch 15/100\n",
      "780/780 - 33s - loss: 0.7526 - val_loss: 0.7218\n",
      "Epoch 16/100\n",
      "780/780 - 33s - loss: 0.7367 - val_loss: 0.7032\n",
      "Epoch 17/100\n",
      "780/780 - 33s - loss: 0.7198 - val_loss: 0.6836\n",
      "Epoch 18/100\n",
      "780/780 - 33s - loss: 0.7011 - val_loss: 0.6609\n",
      "Epoch 19/100\n",
      "780/780 - 33s - loss: 0.6786 - val_loss: 0.6311\n",
      "Epoch 20/100\n",
      "780/780 - 33s - loss: 0.6503 - val_loss: 0.5955\n",
      "Epoch 21/100\n",
      "780/780 - 34s - loss: 0.6142 - val_loss: 0.5519\n",
      "Epoch 22/100\n",
      "780/780 - 33s - loss: 0.5719 - val_loss: 0.5016\n",
      "Epoch 23/100\n",
      "780/780 - 36s - loss: 0.5269 - val_loss: 0.4504\n",
      "Epoch 24/100\n",
      "780/780 - 39s - loss: 0.4868 - val_loss: 0.4082\n",
      "Epoch 25/100\n",
      "780/780 - 41s - loss: 0.4535 - val_loss: 0.3758\n",
      "Epoch 26/100\n",
      "780/780 - 35s - loss: 0.4263 - val_loss: 0.3501\n",
      "Epoch 27/100\n",
      "780/780 - 33s - loss: 0.4013 - val_loss: 0.3292\n",
      "Epoch 28/100\n",
      "780/780 - 34s - loss: 0.3770 - val_loss: 0.3092\n",
      "Epoch 29/100\n",
      "780/780 - 32s - loss: 0.3540 - val_loss: 0.3426\n",
      "Epoch 30/100\n",
      "780/780 - 32s - loss: 0.3269 - val_loss: 0.2702\n",
      "Epoch 31/100\n",
      "780/780 - 32s - loss: 0.3012 - val_loss: 0.2668\n",
      "Epoch 32/100\n",
      "780/780 - 32s - loss: 0.2780 - val_loss: 0.2356\n",
      "Epoch 33/100\n",
      "780/780 - 34s - loss: 0.2545 - val_loss: 0.2210\n",
      "Epoch 34/100\n",
      "780/780 - 33s - loss: 0.2323 - val_loss: 0.2225\n",
      "Epoch 35/100\n",
      "780/780 - 41s - loss: 0.2104 - val_loss: 0.1903\n",
      "Epoch 36/100\n",
      "780/780 - 36s - loss: 0.1925 - val_loss: 0.1745\n",
      "Epoch 37/100\n",
      "780/780 - 35s - loss: 0.1764 - val_loss: 0.1583\n",
      "Epoch 38/100\n",
      "780/780 - 40s - loss: 0.1621 - val_loss: 0.1499\n",
      "Epoch 39/100\n",
      "780/780 - 44s - loss: 0.1500 - val_loss: 0.1406\n",
      "Epoch 40/100\n",
      "780/780 - 35s - loss: 0.1367 - val_loss: 0.1331\n",
      "Epoch 41/100\n",
      "780/780 - 37s - loss: 0.1261 - val_loss: 0.1318\n",
      "Epoch 42/100\n",
      "780/780 - 51s - loss: 0.1155 - val_loss: 0.1219\n",
      "Epoch 43/100\n",
      "780/780 - 36s - loss: 0.1027 - val_loss: 0.1088\n",
      "Epoch 44/100\n",
      "780/780 - 37s - loss: 0.0915 - val_loss: 0.1341\n",
      "Epoch 45/100\n",
      "780/780 - 42s - loss: 0.0871 - val_loss: 0.0977\n",
      "Epoch 46/100\n",
      "780/780 - 40s - loss: 0.0835 - val_loss: 0.1003\n",
      "Epoch 47/100\n",
      "780/780 - 38s - loss: 0.0777 - val_loss: 0.0879\n",
      "Epoch 48/100\n",
      "780/780 - 38s - loss: 0.0736 - val_loss: 0.0798\n",
      "Epoch 49/100\n",
      "780/780 - 43s - loss: 0.0687 - val_loss: 0.0759\n",
      "Epoch 50/100\n",
      "780/780 - 43s - loss: 0.0644 - val_loss: 0.0729\n",
      "Epoch 51/100\n",
      "780/780 - 38s - loss: 0.0594 - val_loss: 0.0682\n",
      "Epoch 52/100\n",
      "780/780 - 35s - loss: 0.0583 - val_loss: 0.0669\n",
      "Epoch 53/100\n",
      "780/780 - 41s - loss: 0.0568 - val_loss: 0.0715\n",
      "Epoch 54/100\n",
      "780/780 - 43s - loss: 0.0563 - val_loss: 0.0700\n",
      "Epoch 55/100\n",
      "780/780 - 39s - loss: 0.0558 - val_loss: 0.0652\n",
      "Epoch 56/100\n",
      "780/780 - 38s - loss: 0.0554 - val_loss: 0.0721\n",
      "Epoch 57/100\n",
      "780/780 - 42s - loss: 0.0545 - val_loss: 0.0649\n",
      "Epoch 58/100\n",
      "780/780 - 37s - loss: 0.0540 - val_loss: 0.0623\n",
      "Epoch 59/100\n",
      "780/780 - 40s - loss: 0.0530 - val_loss: 0.0627\n",
      "Epoch 60/100\n",
      "780/780 - 39s - loss: 0.0527 - val_loss: 0.0642\n",
      "Epoch 61/100\n",
      "780/780 - 33s - loss: 0.0525 - val_loss: 0.0622\n",
      "Epoch 62/100\n",
      "780/780 - 39s - loss: 0.0520 - val_loss: 0.0619\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-e8b5242ba679>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mRNN_model_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized_train_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_train_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "RNN_model_2.fit(normalized_train_X, normalized_train_y, epochs=100, validation_split=0.2, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using temperature and pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[291.87      ],\n",
       "       [291.86818552],\n",
       "       [291.86284446],\n",
       "       ...,\n",
       "       [296.51      ],\n",
       "       [297.09      ],\n",
       "       [296.69      ]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = temp_values.reshape(-1,1) \n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44671, 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization is optional but recommended for neural network as certain \n",
    "# activation functions are sensitive to magnitude of numbers. \n",
    "# normalize the datasets\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1)) #Also try QuantileTransformer\n",
    "new_temp = scaler.fit_transform(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data: (31269, 1)\n",
      "Shape of validation data: (13402, 1)\n"
     ]
    }
   ],
   "source": [
    "n = len(new_temp)\n",
    "train_data = new_temp[0:int(n*0.7),:]\n",
    "val_data = new_temp[int(n*0.7):,:]\n",
    "\n",
    "print(\"Shape of training data: {}\".format(train_data.shape))\n",
    "print(\"Shape of validation data: {}\".format(val_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq_size is the number of previous time steps to use as \n",
    "#input variables to predict the next time period.\n",
    "\n",
    "#creates a dataset where X is the number of passengers at a given time (t, t-1, t-2...) \n",
    "#and Y is the number of passengers at the next time (t + 1).\n",
    "\n",
    "def to_sequences(dataset, window_size=1):\n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    for i in range(len(dataset)-window_size-1):\n",
    "#         print(i)\n",
    "        window = dataset[i:(i+window_size), 0]\n",
    "        x.append(window)\n",
    "#         print(x)\n",
    "        y.append(dataset[i+window_size, 0])\n",
    "#         print(y)\n",
    "        \n",
    "    return np.array(x),np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 1],\n",
       "        [1, 2],\n",
       "        [2, 3],\n",
       "        [3, 4],\n",
       "        [4, 5],\n",
       "        [5, 6],\n",
       "        [6, 7],\n",
       "        [7, 8]]),\n",
       " array([2, 3, 4, 5, 6, 7, 8, 9]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "example_data = example_data.reshape(-1, 1)\n",
    "\n",
    "to_sequences(example_data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set: (31148, 120)\n",
      "Shape of validation set: (13281, 120)\n"
     ]
    }
   ],
   "source": [
    "window_size = 24*5 # Number of time steps to look back \n",
    "#Larger sequences (look further back) may improve forecasting.\n",
    "train_X, train_Y = to_sequences(train_data, window_size)\n",
    "val_X, val_Y = to_sequences(val_data, window_size)\n",
    "\n",
    "#Compare trainX and dataset. You can see that X= values at t, t+1 and t+2\n",
    "#whereas Y is the value that follows, t+3 (since our sequence size is 3)\n",
    "\n",
    "print(\"Shape of training set: {}\".format(train_X.shape))\n",
    "print(\"Shape of validation set: {}\".format(val_X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build deep model...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'window_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-96df78929656>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# create and fit dense model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'window_size' is not defined"
     ]
    }
   ],
   "source": [
    "#Input dimensions are... (N x seq_size)\n",
    "print('Build deep model...')\n",
    "# create and fit dense model\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=window_size, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics = ['acc'])\n",
    "monitor = EarlyStopping(monitor='val_loss', patience=20)\n",
    "print(model.summary()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "974/974 - 3s - loss: 0.0082 - acc: 3.2105e-05 - val_loss: 0.0020 - val_acc: 7.5296e-05\n",
      "Epoch 2/100\n",
      "974/974 - 1s - loss: 0.0021 - acc: 3.2105e-05 - val_loss: 0.0012 - val_acc: 7.5296e-05\n",
      "Epoch 3/100\n",
      "974/974 - 1s - loss: 0.0015 - acc: 3.2105e-05 - val_loss: 9.9564e-04 - val_acc: 7.5296e-05\n",
      "Epoch 4/100\n",
      "974/974 - 1s - loss: 0.0011 - acc: 3.2105e-05 - val_loss: 6.5412e-04 - val_acc: 7.5296e-05\n",
      "Epoch 5/100\n",
      "974/974 - 1s - loss: 9.1442e-04 - acc: 3.2105e-05 - val_loss: 6.3302e-04 - val_acc: 7.5296e-05\n",
      "Epoch 6/100\n",
      "974/974 - 1s - loss: 8.5439e-04 - acc: 3.2105e-05 - val_loss: 6.7036e-04 - val_acc: 7.5296e-05\n",
      "Epoch 7/100\n",
      "974/974 - 1s - loss: 7.7975e-04 - acc: 3.2105e-05 - val_loss: 4.5673e-04 - val_acc: 7.5296e-05\n",
      "Epoch 8/100\n",
      "974/974 - 1s - loss: 7.2087e-04 - acc: 3.2105e-05 - val_loss: 4.6757e-04 - val_acc: 7.5296e-05\n",
      "Epoch 9/100\n",
      "974/974 - 1s - loss: 6.9390e-04 - acc: 3.2105e-05 - val_loss: 4.1291e-04 - val_acc: 7.5296e-05\n",
      "Epoch 10/100\n",
      "974/974 - 1s - loss: 6.6921e-04 - acc: 3.2105e-05 - val_loss: 5.2743e-04 - val_acc: 7.5296e-05\n",
      "Epoch 11/100\n",
      "974/974 - 1s - loss: 6.4867e-04 - acc: 3.2105e-05 - val_loss: 5.6514e-04 - val_acc: 7.5296e-05\n",
      "Epoch 12/100\n",
      "974/974 - 1s - loss: 6.3018e-04 - acc: 3.2105e-05 - val_loss: 4.0714e-04 - val_acc: 7.5296e-05\n",
      "Epoch 13/100\n",
      "974/974 - 1s - loss: 6.2799e-04 - acc: 3.2105e-05 - val_loss: 3.5413e-04 - val_acc: 7.5296e-05\n",
      "Epoch 14/100\n",
      "974/974 - 1s - loss: 6.0562e-04 - acc: 3.2105e-05 - val_loss: 4.6061e-04 - val_acc: 7.5296e-05\n",
      "Epoch 15/100\n",
      "974/974 - 1s - loss: 6.0277e-04 - acc: 3.2105e-05 - val_loss: 6.1680e-04 - val_acc: 7.5296e-05\n",
      "Epoch 16/100\n",
      "974/974 - 1s - loss: 5.8514e-04 - acc: 3.2105e-05 - val_loss: 4.5917e-04 - val_acc: 7.5296e-05\n",
      "Epoch 17/100\n",
      "974/974 - 1s - loss: 5.9613e-04 - acc: 3.2105e-05 - val_loss: 5.5028e-04 - val_acc: 7.5296e-05\n",
      "Epoch 18/100\n",
      "974/974 - 1s - loss: 5.8027e-04 - acc: 3.2105e-05 - val_loss: 3.5805e-04 - val_acc: 7.5296e-05\n",
      "Epoch 19/100\n",
      "974/974 - 1s - loss: 5.7961e-04 - acc: 3.2105e-05 - val_loss: 7.8864e-04 - val_acc: 7.5296e-05\n",
      "Epoch 20/100\n",
      "974/974 - 1s - loss: 5.6648e-04 - acc: 3.2105e-05 - val_loss: 3.4840e-04 - val_acc: 7.5296e-05\n",
      "Epoch 21/100\n",
      "974/974 - 1s - loss: 5.5743e-04 - acc: 3.2105e-05 - val_loss: 3.8347e-04 - val_acc: 7.5296e-05\n",
      "Epoch 22/100\n",
      "974/974 - 1s - loss: 5.7804e-04 - acc: 3.2105e-05 - val_loss: 3.2396e-04 - val_acc: 7.5296e-05\n",
      "Epoch 23/100\n",
      "974/974 - 1s - loss: 5.5478e-04 - acc: 3.2105e-05 - val_loss: 3.7549e-04 - val_acc: 7.5296e-05\n",
      "Epoch 24/100\n",
      "974/974 - 1s - loss: 5.5016e-04 - acc: 3.2105e-05 - val_loss: 4.8712e-04 - val_acc: 7.5296e-05\n",
      "Epoch 25/100\n",
      "974/974 - 1s - loss: 5.4834e-04 - acc: 3.2105e-05 - val_loss: 3.2205e-04 - val_acc: 7.5296e-05\n",
      "Epoch 26/100\n",
      "974/974 - 1s - loss: 5.4800e-04 - acc: 3.2105e-05 - val_loss: 3.7512e-04 - val_acc: 7.5296e-05\n",
      "Epoch 27/100\n",
      "974/974 - 1s - loss: 5.5169e-04 - acc: 3.2105e-05 - val_loss: 3.2114e-04 - val_acc: 7.5296e-05\n",
      "Epoch 28/100\n",
      "974/974 - 1s - loss: 5.4723e-04 - acc: 3.2105e-05 - val_loss: 3.6721e-04 - val_acc: 7.5296e-05\n",
      "Epoch 29/100\n",
      "974/974 - 1s - loss: 5.3977e-04 - acc: 3.2105e-05 - val_loss: 3.1507e-04 - val_acc: 7.5296e-05\n",
      "Epoch 30/100\n",
      "974/974 - 1s - loss: 5.4049e-04 - acc: 3.2105e-05 - val_loss: 3.4757e-04 - val_acc: 7.5296e-05\n",
      "Epoch 31/100\n",
      "974/974 - 1s - loss: 5.3658e-04 - acc: 3.2105e-05 - val_loss: 3.0895e-04 - val_acc: 7.5296e-05\n",
      "Epoch 32/100\n",
      "974/974 - 1s - loss: 5.4935e-04 - acc: 3.2105e-05 - val_loss: 3.2077e-04 - val_acc: 7.5296e-05\n",
      "Epoch 33/100\n",
      "974/974 - 1s - loss: 5.4141e-04 - acc: 3.2105e-05 - val_loss: 5.9479e-04 - val_acc: 7.5296e-05\n",
      "Epoch 34/100\n",
      "974/974 - 1s - loss: 5.4474e-04 - acc: 3.2105e-05 - val_loss: 3.0992e-04 - val_acc: 7.5296e-05\n",
      "Epoch 35/100\n",
      "974/974 - 1s - loss: 5.3168e-04 - acc: 3.2105e-05 - val_loss: 3.3472e-04 - val_acc: 7.5296e-05\n",
      "Epoch 36/100\n",
      "974/974 - 1s - loss: 5.2983e-04 - acc: 3.2105e-05 - val_loss: 3.0955e-04 - val_acc: 7.5296e-05\n",
      "Epoch 37/100\n",
      "974/974 - 1s - loss: 5.4371e-04 - acc: 3.2105e-05 - val_loss: 6.0198e-04 - val_acc: 7.5296e-05\n",
      "Epoch 38/100\n",
      "974/974 - 1s - loss: 5.3208e-04 - acc: 3.2105e-05 - val_loss: 3.4378e-04 - val_acc: 7.5296e-05\n",
      "Epoch 39/100\n",
      "974/974 - 1s - loss: 5.2880e-04 - acc: 3.2105e-05 - val_loss: 4.4613e-04 - val_acc: 7.5296e-05\n",
      "Epoch 40/100\n",
      "974/974 - 1s - loss: 5.3669e-04 - acc: 3.2105e-05 - val_loss: 3.2493e-04 - val_acc: 7.5296e-05\n",
      "Epoch 41/100\n",
      "974/974 - 1s - loss: 5.2982e-04 - acc: 3.2105e-05 - val_loss: 3.8600e-04 - val_acc: 7.5296e-05\n",
      "Epoch 42/100\n",
      "974/974 - 1s - loss: 5.3745e-04 - acc: 3.2105e-05 - val_loss: 3.1999e-04 - val_acc: 7.5296e-05\n",
      "Epoch 43/100\n",
      "974/974 - 1s - loss: 5.3626e-04 - acc: 3.2105e-05 - val_loss: 3.0702e-04 - val_acc: 7.5296e-05\n",
      "Epoch 44/100\n",
      "974/974 - 1s - loss: 5.1863e-04 - acc: 3.2105e-05 - val_loss: 3.2987e-04 - val_acc: 7.5296e-05\n",
      "Epoch 45/100\n",
      "974/974 - 1s - loss: 5.2677e-04 - acc: 3.2105e-05 - val_loss: 3.1462e-04 - val_acc: 7.5296e-05\n",
      "Epoch 46/100\n",
      "974/974 - 1s - loss: 5.2638e-04 - acc: 3.2105e-05 - val_loss: 3.9801e-04 - val_acc: 7.5296e-05\n",
      "Epoch 47/100\n",
      "974/974 - 1s - loss: 5.2419e-04 - acc: 3.2105e-05 - val_loss: 3.0430e-04 - val_acc: 7.5296e-05\n",
      "Epoch 48/100\n",
      "974/974 - 1s - loss: 5.3450e-04 - acc: 3.2105e-05 - val_loss: 3.0648e-04 - val_acc: 7.5296e-05\n",
      "Epoch 49/100\n",
      "974/974 - 1s - loss: 5.3252e-04 - acc: 3.2105e-05 - val_loss: 3.9499e-04 - val_acc: 7.5296e-05\n",
      "Epoch 50/100\n",
      "974/974 - 1s - loss: 5.2660e-04 - acc: 3.2105e-05 - val_loss: 3.4548e-04 - val_acc: 7.5296e-05\n",
      "Epoch 51/100\n",
      "974/974 - 1s - loss: 5.3329e-04 - acc: 3.2105e-05 - val_loss: 3.1521e-04 - val_acc: 7.5296e-05\n",
      "Epoch 52/100\n",
      "974/974 - 1s - loss: 5.2679e-04 - acc: 3.2105e-05 - val_loss: 3.2952e-04 - val_acc: 7.5296e-05\n",
      "Epoch 53/100\n",
      "974/974 - 1s - loss: 5.2506e-04 - acc: 3.2105e-05 - val_loss: 3.6143e-04 - val_acc: 7.5296e-05\n",
      "Epoch 54/100\n",
      "974/974 - 1s - loss: 5.2637e-04 - acc: 3.2105e-05 - val_loss: 3.0982e-04 - val_acc: 7.5296e-05\n",
      "Epoch 55/100\n",
      "974/974 - 1s - loss: 5.2843e-04 - acc: 3.2105e-05 - val_loss: 3.4963e-04 - val_acc: 7.5296e-05\n",
      "Epoch 56/100\n",
      "974/974 - 1s - loss: 5.3191e-04 - acc: 3.2105e-05 - val_loss: 3.6133e-04 - val_acc: 7.5296e-05\n",
      "Epoch 57/100\n",
      "974/974 - 1s - loss: 5.2514e-04 - acc: 3.2105e-05 - val_loss: 4.8125e-04 - val_acc: 7.5296e-05\n",
      "Epoch 58/100\n",
      "974/974 - 1s - loss: 5.2842e-04 - acc: 3.2105e-05 - val_loss: 3.1328e-04 - val_acc: 7.5296e-05\n",
      "Epoch 59/100\n",
      "974/974 - 1s - loss: 5.2064e-04 - acc: 3.2105e-05 - val_loss: 3.1470e-04 - val_acc: 7.5296e-05\n",
      "Epoch 60/100\n",
      "974/974 - 1s - loss: 5.2924e-04 - acc: 3.2105e-05 - val_loss: 3.5644e-04 - val_acc: 7.5296e-05\n",
      "Epoch 61/100\n",
      "974/974 - 1s - loss: 5.2745e-04 - acc: 3.2105e-05 - val_loss: 5.0891e-04 - val_acc: 7.5296e-05\n",
      "Epoch 62/100\n",
      "974/974 - 1s - loss: 5.2352e-04 - acc: 3.2105e-05 - val_loss: 3.6909e-04 - val_acc: 7.5296e-05\n",
      "Epoch 63/100\n",
      "974/974 - 1s - loss: 5.1708e-04 - acc: 3.2105e-05 - val_loss: 6.7804e-04 - val_acc: 7.5296e-05\n",
      "Epoch 64/100\n",
      "974/974 - 1s - loss: 5.2670e-04 - acc: 3.2105e-05 - val_loss: 3.0319e-04 - val_acc: 7.5296e-05\n",
      "Epoch 65/100\n",
      "974/974 - 1s - loss: 5.2045e-04 - acc: 3.2105e-05 - val_loss: 3.1016e-04 - val_acc: 7.5296e-05\n",
      "Epoch 66/100\n",
      "974/974 - 1s - loss: 5.2443e-04 - acc: 3.2105e-05 - val_loss: 3.2228e-04 - val_acc: 7.5296e-05\n",
      "Epoch 67/100\n",
      "974/974 - 1s - loss: 5.2040e-04 - acc: 3.2105e-05 - val_loss: 3.0481e-04 - val_acc: 7.5296e-05\n",
      "Epoch 68/100\n",
      "974/974 - 1s - loss: 5.2135e-04 - acc: 3.2105e-05 - val_loss: 3.0327e-04 - val_acc: 7.5296e-05\n",
      "Epoch 69/100\n",
      "974/974 - 1s - loss: 5.1880e-04 - acc: 3.2105e-05 - val_loss: 3.9723e-04 - val_acc: 7.5296e-05\n",
      "Epoch 70/100\n",
      "974/974 - 1s - loss: 5.1629e-04 - acc: 3.2105e-05 - val_loss: 3.8234e-04 - val_acc: 7.5296e-05\n",
      "Epoch 71/100\n",
      "974/974 - 1s - loss: 5.3600e-04 - acc: 3.2105e-05 - val_loss: 4.2484e-04 - val_acc: 7.5296e-05\n",
      "Epoch 72/100\n",
      "974/974 - 1s - loss: 5.1823e-04 - acc: 3.2105e-05 - val_loss: 3.0629e-04 - val_acc: 7.5296e-05\n",
      "Epoch 73/100\n",
      "974/974 - 1s - loss: 5.1497e-04 - acc: 3.2105e-05 - val_loss: 3.0228e-04 - val_acc: 7.5296e-05\n",
      "Epoch 74/100\n",
      "974/974 - 1s - loss: 5.2177e-04 - acc: 3.2105e-05 - val_loss: 3.7169e-04 - val_acc: 7.5296e-05\n",
      "Epoch 75/100\n",
      "974/974 - 1s - loss: 5.1345e-04 - acc: 3.2105e-05 - val_loss: 4.2784e-04 - val_acc: 7.5296e-05\n",
      "Epoch 76/100\n",
      "974/974 - 1s - loss: 5.1439e-04 - acc: 3.2105e-05 - val_loss: 3.1880e-04 - val_acc: 7.5296e-05\n",
      "Epoch 77/100\n",
      "974/974 - 1s - loss: 5.1761e-04 - acc: 3.2105e-05 - val_loss: 3.5712e-04 - val_acc: 7.5296e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100\n",
      "974/974 - 1s - loss: 5.2234e-04 - acc: 3.2105e-05 - val_loss: 3.0918e-04 - val_acc: 7.5296e-05\n",
      "Epoch 79/100\n",
      "974/974 - 1s - loss: 5.0911e-04 - acc: 3.2105e-05 - val_loss: 3.6213e-04 - val_acc: 7.5296e-05\n",
      "Epoch 80/100\n",
      "974/974 - 1s - loss: 5.1601e-04 - acc: 3.2105e-05 - val_loss: 3.6336e-04 - val_acc: 7.5296e-05\n",
      "Epoch 81/100\n",
      "974/974 - 1s - loss: 5.2739e-04 - acc: 3.2105e-05 - val_loss: 3.0197e-04 - val_acc: 7.5296e-05\n",
      "Epoch 82/100\n",
      "974/974 - 1s - loss: 5.1014e-04 - acc: 3.2105e-05 - val_loss: 3.0542e-04 - val_acc: 7.5296e-05\n",
      "Epoch 83/100\n",
      "974/974 - 1s - loss: 5.1940e-04 - acc: 3.2105e-05 - val_loss: 3.3677e-04 - val_acc: 7.5296e-05\n",
      "Epoch 84/100\n",
      "974/974 - 1s - loss: 5.1795e-04 - acc: 3.2105e-05 - val_loss: 4.7254e-04 - val_acc: 7.5296e-05\n",
      "Epoch 85/100\n",
      "974/974 - 1s - loss: 5.1411e-04 - acc: 3.2105e-05 - val_loss: 3.0204e-04 - val_acc: 7.5296e-05\n",
      "Epoch 86/100\n",
      "974/974 - 1s - loss: 5.1170e-04 - acc: 3.2105e-05 - val_loss: 5.0354e-04 - val_acc: 7.5296e-05\n",
      "Epoch 87/100\n",
      "974/974 - 1s - loss: 5.2392e-04 - acc: 3.2105e-05 - val_loss: 4.0759e-04 - val_acc: 7.5296e-05\n",
      "Epoch 88/100\n",
      "974/974 - 1s - loss: 5.1311e-04 - acc: 3.2105e-05 - val_loss: 3.3104e-04 - val_acc: 7.5296e-05\n",
      "Epoch 89/100\n",
      "974/974 - 1s - loss: 5.1431e-04 - acc: 3.2105e-05 - val_loss: 3.0038e-04 - val_acc: 7.5296e-05\n",
      "Epoch 90/100\n",
      "974/974 - 1s - loss: 5.1488e-04 - acc: 3.2105e-05 - val_loss: 5.6975e-04 - val_acc: 7.5296e-05\n",
      "Epoch 91/100\n",
      "974/974 - 1s - loss: 5.0995e-04 - acc: 3.2105e-05 - val_loss: 3.0276e-04 - val_acc: 7.5296e-05\n",
      "Epoch 92/100\n",
      "974/974 - 1s - loss: 5.0787e-04 - acc: 3.2105e-05 - val_loss: 3.1781e-04 - val_acc: 7.5296e-05\n",
      "Epoch 93/100\n",
      "974/974 - 1s - loss: 5.1863e-04 - acc: 3.2105e-05 - val_loss: 3.5423e-04 - val_acc: 7.5296e-05\n",
      "Epoch 94/100\n",
      "974/974 - 1s - loss: 5.1381e-04 - acc: 3.2105e-05 - val_loss: 2.9975e-04 - val_acc: 7.5296e-05\n",
      "Epoch 95/100\n",
      "974/974 - 1s - loss: 5.0983e-04 - acc: 3.2105e-05 - val_loss: 3.5489e-04 - val_acc: 7.5296e-05\n",
      "Epoch 96/100\n",
      "974/974 - 1s - loss: 5.2320e-04 - acc: 3.2105e-05 - val_loss: 3.4833e-04 - val_acc: 7.5296e-05\n",
      "Epoch 97/100\n",
      "974/974 - 1s - loss: 5.1139e-04 - acc: 3.2105e-05 - val_loss: 3.0684e-04 - val_acc: 7.5296e-05\n",
      "Epoch 98/100\n",
      "974/974 - 1s - loss: 5.1003e-04 - acc: 3.2105e-05 - val_loss: 3.1522e-04 - val_acc: 7.5296e-05\n",
      "Epoch 99/100\n",
      "974/974 - 1s - loss: 5.1526e-04 - acc: 3.2105e-05 - val_loss: 4.7380e-04 - val_acc: 7.5296e-05\n",
      "Epoch 100/100\n",
      "974/974 - 1s - loss: 5.1054e-04 - acc: 3.2105e-05 - val_loss: 8.9317e-04 - val_acc: 7.5296e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8a5962a2b0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, train_Y, validation_data=(val_X, val_Y),\n",
    "          verbose=2, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "\n",
    "trainPredict = model.predict(train_X)\n",
    "valPredict = model.predict(val_X)\n",
    "\n",
    "# Estimate model performance\n",
    "#SInce we used minmaxscaler we can now use scaler.inverse_transform\n",
    "#to invert the transformation.\n",
    "\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY_inverse = scaler.inverse_transform([train_Y])\n",
    "valPredict = scaler.inverse_transform(valPredict)\n",
    "valY_inverse = scaler.inverse_transform([val_Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 13281)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valY_inverse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 1.52 RMSE\n",
      "Test Score: 1.46 RMSE\n"
     ]
    }
   ],
   "source": [
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY_inverse[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "\n",
    "valScore = math.sqrt(mean_squared_error(valY_inverse[0], valPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (valScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RNN(LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set: (31148, 1, 120)\n",
      "Shape of validation set: (13281, 1, 120)\n"
     ]
    }
   ],
   "source": [
    "# Reshape input to be [samples, features, window_size]\n",
    "train_X = np.reshape(train_X, (train_X.shape[0], 1, train_X.shape[1]))\n",
    "val_X = np.reshape(val_X, (val_X.shape[0], 1, val_X.shape[1]))\n",
    "\n",
    "print(\"Shape of training set: {}\".format(train_X.shape))\n",
    "print(\"Shape of validation set: {}\".format(val_X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single LSTM with hidden Dense...\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 8)                 4128      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 4,137\n",
      "Trainable params: 4,137\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('Single LSTM with hidden Dense...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(8, input_shape=(None, window_size)))\n",
    "# model.add(Dense(32))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "#monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=20, \n",
    "#                        verbose=1, mode='auto', restore_best_weights=True)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31148, 1)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_Y.reshape(-1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reshape input to be [samples, features] (Here time_steps = 1)\n",
    "# train_Y = train_Y.reshape(-1, 1)\n",
    "# val_Y = val_Y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "974/974 - 5s - loss: 0.0047 - val_loss: 9.5677e-04\n",
      "Epoch 2/100\n",
      "974/974 - 2s - loss: 0.0011 - val_loss: 6.1673e-04\n",
      "Epoch 3/100\n",
      "974/974 - 2s - loss: 8.7919e-04 - val_loss: 0.0010\n",
      "Epoch 4/100\n",
      "974/974 - 2s - loss: 7.4909e-04 - val_loss: 4.3837e-04\n",
      "Epoch 5/100\n",
      "974/974 - 2s - loss: 7.3648e-04 - val_loss: 4.4876e-04\n",
      "Epoch 6/100\n",
      "974/974 - 2s - loss: 6.7064e-04 - val_loss: 4.0169e-04\n",
      "Epoch 7/100\n",
      "974/974 - 2s - loss: 6.5661e-04 - val_loss: 3.7889e-04\n",
      "Epoch 8/100\n",
      "974/974 - 2s - loss: 6.2571e-04 - val_loss: 3.7376e-04\n",
      "Epoch 9/100\n",
      "974/974 - 2s - loss: 5.9987e-04 - val_loss: 4.2064e-04\n",
      "Epoch 10/100\n",
      "974/974 - 2s - loss: 6.0999e-04 - val_loss: 3.4612e-04\n",
      "Epoch 11/100\n",
      "974/974 - 2s - loss: 5.9407e-04 - val_loss: 3.7807e-04\n",
      "Epoch 12/100\n",
      "974/974 - 2s - loss: 5.7702e-04 - val_loss: 3.4591e-04\n",
      "Epoch 13/100\n",
      "974/974 - 2s - loss: 5.9316e-04 - val_loss: 5.1105e-04\n",
      "Epoch 14/100\n",
      "974/974 - 2s - loss: 5.6586e-04 - val_loss: 4.5605e-04\n",
      "Epoch 15/100\n",
      "974/974 - 2s - loss: 5.7049e-04 - val_loss: 3.5722e-04\n",
      "Epoch 16/100\n",
      "974/974 - 2s - loss: 5.6941e-04 - val_loss: 5.2389e-04\n",
      "Epoch 17/100\n",
      "974/974 - 2s - loss: 5.6083e-04 - val_loss: 5.1490e-04\n",
      "Epoch 18/100\n",
      "974/974 - 2s - loss: 5.6519e-04 - val_loss: 6.2669e-04\n",
      "Epoch 19/100\n",
      "974/974 - 2s - loss: 5.5544e-04 - val_loss: 9.8204e-04\n",
      "Epoch 20/100\n",
      "974/974 - 2s - loss: 5.6521e-04 - val_loss: 3.3951e-04\n",
      "Epoch 21/100\n",
      "974/974 - 2s - loss: 5.3983e-04 - val_loss: 4.7222e-04\n",
      "Epoch 22/100\n",
      "974/974 - 2s - loss: 5.5099e-04 - val_loss: 3.7475e-04\n",
      "Epoch 23/100\n",
      "974/974 - 2s - loss: 5.4204e-04 - val_loss: 3.2058e-04\n",
      "Epoch 24/100\n",
      "974/974 - 2s - loss: 5.6242e-04 - val_loss: 3.6463e-04\n",
      "Epoch 25/100\n",
      "974/974 - 2s - loss: 5.5615e-04 - val_loss: 3.2870e-04\n",
      "Epoch 26/100\n",
      "974/974 - 2s - loss: 5.4039e-04 - val_loss: 4.9886e-04\n",
      "Epoch 27/100\n",
      "974/974 - 2s - loss: 5.3325e-04 - val_loss: 3.2263e-04\n",
      "Epoch 28/100\n",
      "974/974 - 2s - loss: 5.4250e-04 - val_loss: 8.8957e-04\n",
      "Epoch 29/100\n",
      "974/974 - 2s - loss: 5.3620e-04 - val_loss: 3.5745e-04\n",
      "Epoch 30/100\n",
      "974/974 - 2s - loss: 5.3352e-04 - val_loss: 4.0146e-04\n",
      "Epoch 31/100\n",
      "974/974 - 2s - loss: 5.2645e-04 - val_loss: 4.0336e-04\n",
      "Epoch 32/100\n",
      "974/974 - 2s - loss: 5.2452e-04 - val_loss: 4.6395e-04\n",
      "Epoch 33/100\n",
      "974/974 - 2s - loss: 5.3267e-04 - val_loss: 3.1837e-04\n",
      "Epoch 34/100\n",
      "974/974 - 2s - loss: 5.2742e-04 - val_loss: 3.2662e-04\n",
      "Epoch 35/100\n",
      "974/974 - 2s - loss: 5.2125e-04 - val_loss: 3.4065e-04\n",
      "Epoch 36/100\n",
      "974/974 - 2s - loss: 5.1855e-04 - val_loss: 3.3661e-04\n",
      "Epoch 37/100\n",
      "974/974 - 2s - loss: 5.2043e-04 - val_loss: 6.0868e-04\n",
      "Epoch 38/100\n",
      "974/974 - 2s - loss: 5.2288e-04 - val_loss: 3.2687e-04\n",
      "Epoch 39/100\n",
      "974/974 - 2s - loss: 5.3124e-04 - val_loss: 5.4120e-04\n",
      "Epoch 40/100\n",
      "974/974 - 2s - loss: 5.1986e-04 - val_loss: 5.6048e-04\n",
      "Epoch 41/100\n",
      "974/974 - 2s - loss: 5.1630e-04 - val_loss: 3.1331e-04\n",
      "Epoch 42/100\n",
      "974/974 - 2s - loss: 5.1647e-04 - val_loss: 3.1469e-04\n",
      "Epoch 43/100\n",
      "974/974 - 2s - loss: 5.1463e-04 - val_loss: 3.0660e-04\n",
      "Epoch 44/100\n",
      "974/974 - 2s - loss: 5.2241e-04 - val_loss: 3.1142e-04\n",
      "Epoch 45/100\n",
      "974/974 - 2s - loss: 5.1170e-04 - val_loss: 3.1405e-04\n",
      "Epoch 46/100\n",
      "974/974 - 2s - loss: 5.1286e-04 - val_loss: 3.1298e-04\n",
      "Epoch 47/100\n",
      "974/974 - 2s - loss: 5.1258e-04 - val_loss: 3.1072e-04\n",
      "Epoch 48/100\n",
      "974/974 - 2s - loss: 5.1896e-04 - val_loss: 3.2467e-04\n",
      "Epoch 49/100\n",
      "974/974 - 2s - loss: 5.0712e-04 - val_loss: 3.8388e-04\n",
      "Epoch 50/100\n",
      "974/974 - 2s - loss: 5.1519e-04 - val_loss: 4.4111e-04\n",
      "Epoch 51/100\n",
      "974/974 - 2s - loss: 5.0358e-04 - val_loss: 6.1840e-04\n",
      "Epoch 52/100\n",
      "974/974 - 2s - loss: 5.0431e-04 - val_loss: 3.6777e-04\n",
      "Epoch 53/100\n",
      "974/974 - 2s - loss: 5.0522e-04 - val_loss: 3.7253e-04\n",
      "Epoch 54/100\n",
      "974/974 - 2s - loss: 5.0777e-04 - val_loss: 3.4998e-04\n",
      "Epoch 55/100\n",
      "974/974 - 2s - loss: 4.9841e-04 - val_loss: 5.1290e-04\n",
      "Epoch 56/100\n",
      "974/974 - 3s - loss: 4.9890e-04 - val_loss: 3.1328e-04\n",
      "Epoch 57/100\n",
      "974/974 - 3s - loss: 5.0495e-04 - val_loss: 4.1125e-04\n",
      "Epoch 58/100\n",
      "974/974 - 3s - loss: 4.9805e-04 - val_loss: 3.0334e-04\n",
      "Epoch 59/100\n",
      "974/974 - 3s - loss: 4.9934e-04 - val_loss: 3.3131e-04\n",
      "Epoch 60/100\n",
      "974/974 - 2s - loss: 5.0148e-04 - val_loss: 3.2066e-04\n",
      "Epoch 61/100\n",
      "974/974 - 2s - loss: 4.9905e-04 - val_loss: 3.8912e-04\n",
      "Epoch 62/100\n",
      "974/974 - 2s - loss: 5.0022e-04 - val_loss: 3.2998e-04\n",
      "Epoch 63/100\n",
      "974/974 - 2s - loss: 4.9833e-04 - val_loss: 3.9635e-04\n",
      "Epoch 64/100\n",
      "974/974 - 2s - loss: 4.9807e-04 - val_loss: 3.2137e-04\n",
      "Epoch 65/100\n",
      "974/974 - 2s - loss: 5.0044e-04 - val_loss: 4.9209e-04\n",
      "Epoch 66/100\n",
      "974/974 - 2s - loss: 4.8922e-04 - val_loss: 3.7777e-04\n",
      "Epoch 67/100\n",
      "974/974 - 2s - loss: 5.0093e-04 - val_loss: 3.0396e-04\n",
      "Epoch 68/100\n",
      "974/974 - 2s - loss: 4.9102e-04 - val_loss: 3.4708e-04\n",
      "Epoch 69/100\n",
      "974/974 - 2s - loss: 4.9489e-04 - val_loss: 4.4731e-04\n",
      "Epoch 70/100\n",
      "974/974 - 2s - loss: 4.9652e-04 - val_loss: 3.8825e-04\n",
      "Epoch 71/100\n",
      "974/974 - 2s - loss: 5.0372e-04 - val_loss: 3.5359e-04\n",
      "Epoch 72/100\n",
      "974/974 - 2s - loss: 4.8790e-04 - val_loss: 3.0433e-04\n",
      "Epoch 73/100\n",
      "974/974 - 2s - loss: 4.9821e-04 - val_loss: 3.1898e-04\n",
      "Epoch 74/100\n",
      "974/974 - 2s - loss: 4.9050e-04 - val_loss: 3.1094e-04\n",
      "Epoch 75/100\n",
      "974/974 - 2s - loss: 4.9300e-04 - val_loss: 3.1137e-04\n",
      "Epoch 76/100\n",
      "974/974 - 2s - loss: 4.9387e-04 - val_loss: 3.2098e-04\n",
      "Epoch 77/100\n",
      "974/974 - 2s - loss: 4.9080e-04 - val_loss: 2.9885e-04\n",
      "Epoch 78/100\n",
      "974/974 - 2s - loss: 4.9334e-04 - val_loss: 3.0374e-04\n",
      "Epoch 79/100\n",
      "974/974 - 2s - loss: 4.8630e-04 - val_loss: 3.1725e-04\n",
      "Epoch 80/100\n",
      "974/974 - 2s - loss: 4.9302e-04 - val_loss: 3.0621e-04\n",
      "Epoch 81/100\n",
      "974/974 - 2s - loss: 4.8890e-04 - val_loss: 4.8596e-04\n",
      "Epoch 82/100\n",
      "974/974 - 2s - loss: 4.9385e-04 - val_loss: 3.0339e-04\n",
      "Epoch 83/100\n",
      "974/974 - 2s - loss: 4.8669e-04 - val_loss: 5.9230e-04\n",
      "Epoch 84/100\n",
      "974/974 - 2s - loss: 4.8516e-04 - val_loss: 3.4214e-04\n",
      "Epoch 85/100\n",
      "974/974 - 2s - loss: 4.8965e-04 - val_loss: 3.2651e-04\n",
      "Epoch 86/100\n",
      "974/974 - 2s - loss: 4.8619e-04 - val_loss: 3.3712e-04\n",
      "Epoch 87/100\n",
      "974/974 - 2s - loss: 4.9866e-04 - val_loss: 3.8986e-04\n",
      "Epoch 88/100\n",
      "974/974 - 2s - loss: 4.8474e-04 - val_loss: 3.3213e-04\n",
      "Epoch 89/100\n",
      "974/974 - 2s - loss: 4.8219e-04 - val_loss: 3.7082e-04\n",
      "Epoch 90/100\n",
      "974/974 - 2s - loss: 4.9261e-04 - val_loss: 3.6743e-04\n",
      "Epoch 91/100\n",
      "974/974 - 2s - loss: 4.7992e-04 - val_loss: 3.6940e-04\n",
      "Epoch 92/100\n",
      "974/974 - 3s - loss: 4.8404e-04 - val_loss: 7.0296e-04\n",
      "Epoch 93/100\n",
      "974/974 - 2s - loss: 4.9157e-04 - val_loss: 3.1455e-04\n",
      "Epoch 94/100\n",
      "974/974 - 2s - loss: 4.8124e-04 - val_loss: 3.3636e-04\n",
      "Epoch 95/100\n",
      "974/974 - 2s - loss: 4.7301e-04 - val_loss: 3.5674e-04\n",
      "Epoch 96/100\n",
      "974/974 - 3s - loss: 4.8031e-04 - val_loss: 4.8804e-04\n",
      "Epoch 97/100\n",
      "974/974 - 2s - loss: 4.7760e-04 - val_loss: 3.4926e-04\n",
      "Epoch 98/100\n",
      "974/974 - 2s - loss: 4.7593e-04 - val_loss: 2.9898e-04\n",
      "Epoch 99/100\n",
      "974/974 - 2s - loss: 4.8464e-04 - val_loss: 3.0432e-04\n",
      "Epoch 100/100\n",
      "974/974 - 2s - loss: 4.9269e-04 - val_loss: 3.0762e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8a5e933e80>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, train_Y, validation_data=(val_X, val_Y),\n",
    "          verbose=2, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "\n",
    "trainPredict = model.predict(train_X)\n",
    "valPredict = model.predict(val_X)\n",
    "\n",
    "# Estimate model performance\n",
    "#SInce we used minmaxscaler we can now use scaler.inverse_transform\n",
    "#to invert the transformation.\n",
    "\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([train_Y])\n",
    "valPredict = scaler.inverse_transform(valPredict)\n",
    "valY = scaler.inverse_transform([val_Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 1.03 RMSE\n",
      "Test Score: 0.86 RMSE\n"
     ]
    }
   ],
   "source": [
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "\n",
    "valScore = math.sqrt(mean_squared_error(valY[0], valPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (valScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(df_for_training)\n",
    "df_for_training_scaled = scaler.transform(df_for_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_for_training_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX shape == (857, 120, 3).\n",
      "trainY shape == (857, 1).\n"
     ]
    }
   ],
   "source": [
    "#As required for LSTM networks, we require to reshape an input data into n_samples x timesteps x n_features. \n",
    "#In this example, the n_features is 2. We will make timesteps = 3. \n",
    "#With this, the resultant n_samples is 5 (as the input data has 9 rows).\n",
    "trainX = []\n",
    "trainY = []\n",
    "\n",
    "n_future = 24   # Number of days we want to predict into the future\n",
    "n_past = 24*5     # Number of past days we want to use to predict the future\n",
    "\n",
    "for i in range(n_past, len(df_for_training_scaled) - n_future +1):\n",
    "    trainX.append(df_for_training_scaled[i - n_past:i, 0:df_for_training.shape[1]])\n",
    "    trainY.append(df_for_training_scaled[i + n_future - 1:i + n_future, 0])\n",
    "\n",
    "trainX, trainY = np.array(trainX), np.array(trainY)\n",
    "\n",
    "print('trainX shape == {}.'.format(trainX.shape))\n",
    "print('trainY shape == {}.'.format(trainY.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_17 (LSTM)               (None, 120, 8)            384       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 120, 1)            9         \n",
      "=================================================================\n",
      "Total params: 393\n",
      "Trainable params: 393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(8, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=True))\n",
    "model.add(Dense(trainY.shape[1]))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 2s 137ms/step - loss: 0.8938 - val_loss: 1.4555\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 63ms/step - loss: 0.7869 - val_loss: 1.4548\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 67ms/step - loss: 0.7750 - val_loss: 1.4560\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 63ms/step - loss: 0.7594 - val_loss: 1.4571\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 67ms/step - loss: 0.7217 - val_loss: 1.4607\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 65ms/step - loss: 0.7764 - val_loss: 1.4655\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 64ms/step - loss: 0.6987 - val_loss: 1.4694\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 67ms/step - loss: 0.7556 - val_loss: 1.4747\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 67ms/step - loss: 0.7517 - val_loss: 1.4782\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 66ms/step - loss: 0.7644 - val_loss: 1.4837\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "history = model.fit(trainX, trainY, epochs=10, batch_size=128, validation_split=0.3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Forecasting...\n",
    "# #Start with the last day in training date and predict future...\n",
    "# n_future=24  #Redefining n_future to extend prediction dates beyond original n_future dates...\n",
    "# train_dates = pd.to_datetime(df['datetime'])\n",
    "# forecast_period_dates = pd.date_range(list(train_dates)[-1], periods=n_future, freq='1d').tolist()\n",
    "\n",
    "forecast = model.predict(trainX) #forecast \n",
    "\n",
    "#Perform inverse transformation to rescale back to original range\n",
    "#Since we used 5 variables for transform, the inverse expects same dimensions\n",
    "#Therefore, let us copy our values 5 times and discard them after inverse transform\n",
    "forecast_copies = np.repeat(forecast, df_for_training.shape[1], axis=-1)\n",
    "y_pred_future = scaler.inverse_transform(forecast_copies)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(857, 3)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_future.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(857, 120, 1)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.499452  , 0.5264093 , 0.57623946, ..., 0.4296081 , 0.41041123,\n",
       "       0.4044888 ])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (93444,) (3,) (93444,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-197-e51df0142a9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mforecast_copies_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_for_training\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_pred_future_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforecast_copies_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m    935\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_std\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m                 \u001b[0mX\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_mean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (93444,) (3,) (93444,) "
     ]
    }
   ],
   "source": [
    "forecast_copies_Y = np.repeat(train_Y, df_for_training.shape[1], axis=-1)\n",
    "y_pred_future_Y = scaler.inverse_transform(forecast_copies_Y)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Data must be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-182-8160fe94434d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mforecast_dates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf_forecast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforecast_dates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Temperature'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_pred_future\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdf_forecast\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_forecast\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         ]\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# don't force copy because getting jammed in an ndarray anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_homogenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_homogenize\u001b[0;34m(data, index, dtype)\u001b[0m\n\u001b[1;32m    349\u001b[0m                     \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_multiget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m             val = sanitize_array(\n\u001b[0m\u001b[1;32m    352\u001b[0m                 \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             )\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0msubarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data must be 1-dimensional\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray_tuplesafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Data must be 1-dimensional"
     ]
    }
   ],
   "source": [
    "# Convert timestamp to date\n",
    "forecast_dates = []\n",
    "for time_i in forecast_period_dates:\n",
    "    forecast_dates.append(time_i.date())\n",
    "    \n",
    "df_forecast = pd.DataFrame({'Date':np.array(forecast_dates), 'Temperature':y_pred_future})\n",
    "df_forecast['Date']=pd.to_datetime(df_forecast['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = df[['Date', 'Open']]\n",
    "original['Date']=pd.to_datetime(original['Date'])\n",
    "original = original.loc[original['Date'] >= '2020-5-1']\n",
    "\n",
    "sns.lineplot(original['Date'], original['Open'])\n",
    "sns.lineplot(df_forecast['Date'], df_forecast['Open'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44551, 120, 2)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44551, 1)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44551, 120, 1)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast = model.predict(trainX) #forecast \n",
    "forecast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. Estimator expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-d2b78df778df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#Therefore, let us copy our values 5 times and discard them after inverse transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mforecast_copies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforecast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_for_training\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0my_pred_future\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforecast_copies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         X = check_array(X, copy=self.copy, dtype=FLOAT_DTYPES,\n\u001b[0m\u001b[1;32m    459\u001b[0m                         force_all_finite=\"allow-nan\")\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    657\u001b[0m                     \"into decimal numbers with dtype='numeric'\") from e\n\u001b[1;32m    658\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n\u001b[0m\u001b[1;32m    660\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. Estimator expected <= 2."
     ]
    }
   ],
   "source": [
    "#Perform inverse transformation to rescale back to original range\n",
    "#Since we used 5 variables for transform, the inverse expects same dimensions\n",
    "#Therefore, let us copy our values 5 times and discard them after inverse transform\n",
    "forecast_copies = np.repeat(forecast, df_for_training.shape[1], axis=-1)\n",
    "y_pred_future = scaler.inverse_transform(forecast_copies)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
